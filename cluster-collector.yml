apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: cluster-otel-collector
  namespace: kube-monitoring
  labels:
    app.kubernetes.io/instance: cluster-otel-collector
    app_kind: "observability"
    app: "cluster-otel-collector"
    criticality: "c2"
    customer_facing: "false"
    exposition: "private"
spec:
  mode: deployment
  serviceAccount: otelcontribcol
  # Node affinity is conceptually similar to nodeSelector
  # it allows you to constrain which nodes your pod is eligible to be scheduled on, based on labels on the node.
  # Node Taint + Pod Toleration + Pod Node Affinity is the final combo to ensure that specific Pods Node will run on the same page.   
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: 'node.keplr.io/workload'
            operator: In
            values:
              - stable
  resources:
    requests:
      memory: 100Mi
    limits:
      memory: 1Gi
  env:
    - name: DD_SERVICE
      value: cluster-otel-collector
    - name: HONEYCOMB_API_KEY
      valueFrom:
        secretKeyRef:
          name: honeycomb
          key: api-key
    - name: K8S_NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
  config: |
    receivers:
      k8sobjects:
        auth_type: serviceAccount
        objects:
          - name: events
            mode: watch
      k8s_cluster:
        collection_interval: 40s
        node_conditions_to_report: [Ready, MemoryPressure, DiskPressure, NetworkUnavailable]
        allocatable_types_to_report: [cpu, memory, storage, ephemeral-storage]
        metrics:
          k8s.resource_quota.hard_limit:
            enabled: false
          k8s.resource_quota.used:
            enabled: false
          k8s.namespace.phase:
            enabled: false
          k8s.container.storage_request:
            enabled: false
          k8s.container.storage_limit:
            enabled: false
          k8s.container.ephemeralstorage_request:
            enabled: false
          k8s.container.ephemeralstorage_limit:
            enabled: false
          k8s.container.cpu_request:
            enabled: false
          k8s.container.memory_request:
            enabled: false
          k8s.container.cpu_limit:
            enabled: false
          k8s.container.ready:
            enabled: false
          k8s.replicaset.desired:
            enabled: false
          k8s.replicaset.available:
            enabled: false
          k8s.job.max_parallel_pods:
            enabled: false
          k8s.job.desired_successful_pods:
            enabled: false
          k8s.job.successful_pods:
            enabled: false 
          k8s.job.active_pods:
            enabled: false
          k8s.hpa.max_replicas:
            enabled: false
          k8s.hpa.min_replicas:
            enabled: false
          k8s.hpa.current_replicas:
            enabled: false
          k8s.replication_controller.desired:
            enabled: false
          k8s.replication_controller.available:
            enabled: false
          k8s.statefulset.updated_pods:
            enabled: false
          openshift.clusterquota.limit:
            enabled: false
          openshift.clusterquota.used:
            enabled: false
          openshift.appliedclusterquota.limit:
            enabled: false
          openshift.appliedclusterquota.used:
            enabled: false

      
    processors:
      k8sattributes:
        passthrough: false
        auth_type: serviceAccount
        filter:
          node_from_env_var: $K8S_NODE_NAME
        pod_association:
          - sources: # below association matches for pair `k8s.pod.name` and `k8s.namespace.name`. it will look at the corresponding datapoint & try to match it to the pod attributes.
            - from: resource_attribute
              name: k8s.pod.name
            - from: resource_attribute
              name: k8s.namespace.name
        extract:
          metadata:
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.deployment.name
            - k8s.namespace.name
            - k8s.node.name
            - k8s.pod.start_time

      resourcedetection:
        detectors: [env, gcp]
        timeout: 2s
        override: true
      
      memory_limiter:
        # drop metrics if memory usage gets too high
        check_interval: 1s
        limit_percentage: 70
        spike_limit_percentage: 30
      
      batch:
        # batch metrics before sending to reduce API usage
        #send_batch_size: 8192
        timeout: 5s
      
      groupbyattrs:
        keys:
          - host.id
          - k8s.node.name
          - k8s.deployment.name
          - k8s.pod.name
          - k8s.pod.uid
          - container.id
          - k8s.container.name
      
      transform/events:
        error_mode: ignore
        log_statements:
          - context: log
            statements:
              # adds a new watch-type attribute from the body if it exists
              - set(attributes["watch-type"], body["type"]) where IsMap(body) and body["type"] != nil

              # create new attributes from the body if the body is an object
              - merge_maps(attributes, body, "upsert") where IsMap(body) and body["object"] == nil
              - merge_maps(attributes, body["object"], "upsert") where IsMap(body) and body["object"] != nil

              # Transform the attributes so that the log events use the k8s.* semantic conventions
              #- merge_maps(attributes, attributes["metadata"], "upsert") where IsMap(attributes["metadata"])
              - set(attributes["k8s.pod.name"], attributes["involvedObject"]["name"]) where attributes["involvedObject"]["kind"] == "Pod"
              - set(attributes["k8s.node.name"], attributes["involvedObject"]["name"]) where attributes["involvedObject"]["kind"] == "Node"
              - set(attributes["k8s.job.name"], attributes["involvedObject"]["name"]) where attributes["involvedObject"]["kind"] == "Job"
              - set(attributes["k8s.cronjob.name"], attributes["involvedObject"]["name"]) where attributes["involvedObject"]["kind"] == "CronJob"
              - set(attributes["k8s.namespace.name"], attributes["involvedObject"]["namespace"]) where attributes["involvedObject"]["kind"] == "Pod" or attributes["involvedObject"]["kind"] == "Job" or attributes["regarding"]["kind"] == "CronJob"

              # Transform the type attributes into OpenTelemetry Severity types.
              - set(severity_text, attributes["type"]) where attributes["type"] == "Normal" or attributes["type"] == "Warning"
              - set(severity_number, SEVERITY_NUMBER_INFO) where attributes["type"] == "Normal"
              - set(severity_number, SEVERITY_NUMBER_WARN) where attributes["type"] == "Warning"

    exporters:
      otlp/k8s-metrics:
        endpoint: "api.honeycomb.io:443"
        headers:
          "x-honeycomb-team": "${env:HONEYCOMB_API_KEY}"
          "x-honeycomb-dataset": "k8s-metrics"
      otlp/k8s-events:
        endpoint: "api.honeycomb.io:443"
        headers:
          "x-honeycomb-team": "${env:HONEYCOMB_API_KEY}"
          "x-honeycomb-dataset": "k8s-events"

    service:
      pipelines:
        logs:
          receivers: [k8sobjects]
          processors:
            - memory_limiter
            - transform/events
            - batch
          exporters: [otlp/k8s-events]
        metrics:
          receivers: [k8s_cluster]
          processors:
            - memory_limiter
            - k8sattributes
            - resourcedetection
            - groupbyattrs
            - batch
          exporters: [otlp/k8s-metrics]
      telemetry:
        logs:
          level: info
          #development: true
          encoding: "json"
